---
title: "[RL/01] Actor Critic"
tags: [RL]
pages: papers
style: border  # fill / border 
img: ../assets/images/RL/ActorCritic/image.png
description: "Actor Critic ê°œë…, A2C, A3C, ì½”ë“œ êµ¬í˜„<br>-<br>Actor Critic Notion, A2C, A3C, Code"
---

<!-- í•œêµ­ì–´ ì½˜í…ì¸  -->
{% capture ko_content %}
## [BG] ì •ì±… ì¤‘ì‹¬ RL, ê°€ì¹˜ ì¤‘ì‹¬ RL 
ê°•í™”í•™ìŠµ êµ¬í˜„ì€ í¬ê²Œ **ì •ì±…** ì¤‘ì  ë°©ë²•ë¡ , **ê°€ì¹˜** ì¤‘ì  ë°©ë²•ë¡  ë‘ ê°œë¡œ ë‚˜ë‰œë‹¤.  

### ì •ì±… ì¤‘ì‹¬ RL
ë¨¼ì € ì •ì±… ì¤‘ì  ë°©ë²•ë¡ ì€ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê·¼ì‚¬í•˜ì§€ ì•Šê³ , ìµœì  ì •ì±…ì„ ì§ì ‘ì ìœ¼ë¡œ ê·¼ì‚¬í•œë‹¤. ì •ì±… ì¤‘ì  ë°©ë²•ë¡ ì€ $\pi_\theta(a|s)$,  $\theta$ ë¡œ ì¡°ì •ê°€ëŠ¥í•œ ì •ì±… í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ê·¸ë˜ë””ì–¸íŠ¸ ì¡°ì •ì„ í†µí•´ ìµœì  ì •ì±… í•¨ìˆ˜ë¥¼ êµ¬í•œë‹¤. ì´ëŠ” ê¸°ëŒ€ ëˆ„ì  ë³´ìƒì„ ê·¹ëŒ€í™”í•˜ëŠ” ê°•í™”í•™ìŠµì˜ í•™ìŠµ ëª©í‘œë¥¼ ì§ì ‘ì ìœ¼ë¡œ ìµœì í™”í•œë‹¤ëŠ” ì ì—ì„œ ì´ë¡ ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. í•˜ì§€ë§Œ ê° ìƒíƒœ-í–‰ë™ ìŒì˜ ì •í™•í•œ ê°€ì¹˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ëª»í•˜ê³ , ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ì¶”ì •ì¹˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê·¸ë˜ë””ì–¸íŠ¸ì˜ ë¶„ì‚°ì´ ì»¤ì§ˆ ìˆ˜ ìˆìœ¼ë©°, í•™ìŠµ ì•ˆì •ì„±ì„ ì €í•´í•  ìˆ˜ ìˆë‹¤. 

#### ì˜ˆì‹œ 

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot Q_{\pi_\theta}(s, a) \right]$$

- í´ë¦¬ì‰¬ ê·¸ë˜ë””ì–¸íŠ¸ì˜ ì •ì±… ì‹ ê²½ë§ ì—…ë°ì´íŠ¸ ì‹  

ëŒ€í‘œì ì¸ ì •ì±… ê¸°ë°˜ RLì¸ Reinforce ì•Œê³ ë¦¬ì¦˜ì€ ì•Œ ìˆ˜ ì—†ëŠ” $Q(s, a)$ ì„ ë°˜í™˜ê°’ì¸ $G_t$ë¡œ ì¹˜í™˜í•œë‹¤. 

### ê°€ì¹˜ ì¤‘ì‹¬ RL 
ê°€ì¹˜ ì¤‘ì‹¬ ê°•í™”í•™ìŠµ ë°©ë²•ë¡ ì€ ì •ì±…ì„ ëª…ì‹œì ìœ¼ë¡œ íŒŒë¼ë¯¸í„°í™”í•˜ì§€ ì•Šê³ , ìƒíƒœ-í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜  $Q(s, a)$ ë¥¼ ê·¼ì‚¬í•˜ì—¬ ê°„ì ‘ì ìœ¼ë¡œ ìµœì  ì •ì±…ì„ ìœ ë„í•œë‹¤. ê°€ì¹˜ í•¨ìˆ˜ê°€ ì˜ ê·¼ì‚¬ëœë‹¤ë©´, ì—ì´ì „íŠ¸ëŠ” ê° ìƒíƒœì—ì„œ ê°€ì¥ ë†’ì€ ê°€ì¹˜ë¥¼ ê°–ëŠ” í–‰ë™ì„ ì„ íƒí•˜ëŠ” ì •ì±…  $\pi(s) = \arg\max_a Q(s, a)$ ì„ ë”°ë¦„ìœ¼ë¡œì¨ ìµœì ì˜ í–‰ë™ ì „ëµì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì€ ì¼ë°˜ì ìœ¼ë¡œ Temporal Difference(ì‹œê°„ì°¨) í•™ìŠµì„ ì‚¬ìš©í•˜ì—¬ ë²¨ë§Œ ë°©ì •ì‹ì„ ê·¼ì‚¬í•˜ê³ , ë°˜ë³µì ì¸ ì˜ˆì¸¡-ì—…ë°ì´íŠ¸ ê³¼ì •ì„ í†µí•´ ìˆ˜ë ´í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì •ì±…ì„ ì§ì ‘ ìµœì í™”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— í™•ë¥ ì  ì •ì±… í‘œí˜„ì´ ì–´ë µê³ , ì ì ˆí•œ íƒí—˜(exploration)ì´ ì™¸ë¶€ ë©”ì»¤ë‹ˆì¦˜(ì˜ˆ: Îµ-greedy)ì— ì˜í•´ ë³´ì¥ë˜ì–´ì•¼ ì›í™œí•œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.

---
### ğŸ’¡ Temporal Difference(ì‹œê°„ì°¨) í•™ìŠµ

![](../assets/images/RL/ActorCritic/image1.png)  


ì‹œê°„ì°¨ í•™ìŠµ(Temporal Difference Learning)ì€ model-free í™˜ê²½ì—ì„œ í‘œë³¸ ê²½ë¡œë¥¼ ì´ìš©í•´ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆëŠ” Monte Carlo ë°©ì‹ì˜ ì¥ì ê³¼, ëª¨ë“  ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë˜ì§€ ì•Šì•„ë„ ì‹œê°„ ë‹¨ê³„ë³„ë¡œ ê°’ì„ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆëŠ” Dynamic Programmingì˜ ì¥ì ì„ ê²°í•©í•œ ë°©ë²•ë¡ ì´ë‹¤. ì´ ë°©ë²•ì€ ë²¨ë§Œ ê¸°ëŒ€ ë°©ì •ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°,

$$v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t = s]$$

ì˜ ìš°ë³€ì— í•´ë‹¹í•˜ëŠ” ê°’  $R_{t+1} + \gamma V_{\pi}(S_{t+1})$ ì„ TD íƒ€ê²Ÿìœ¼ë¡œ ì‚¼ì•„, ì˜ˆì¸¡ê°’ê³¼ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì—¬ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.  

ì¼ë°˜ì ìœ¼ë¡œ ì‹œê°„ì°¨ í•™ìŠµì—ì„œëŠ” ë‹¤ìŒ ìƒíƒœì—ì„œì˜ ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ê°€ì¹˜ ì¶”ì •ì„ ì—…ë°ì´íŠ¸í•˜ëŠ”ë°, ì´ë•Œ 1ìŠ¤í… TD íƒ€ê²Ÿì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì„ TD(0) ë¼ê³  í•œë‹¤.
ë°˜ë©´, ë¯¸ë˜  n  ìŠ¤í…ì— ê±¸ì¹œ ì‹¤ì œ ë³´ìƒ ì‹œí€€ìŠ¤ë¥¼ ì´ìš©í•´ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì‹ì„ TD(n) ì´ë¼ê³  í•˜ë©°,
 $n \to \infty$ ì¸ ê²½ìš°ëŠ” ì—í”¼ì†Œë“œ ì „ì²´ë¥¼ í™œìš©í•˜ëŠ” Monte Carlo ë°©ì‹, ì¦‰ TD(1) ì´ ëœë‹¤.

### Monte Carlo vs Bootstrap in RL 
ë¶€íŠ¸ìŠ¤íŠ¸ë©ì€ ë‹¤ìŒ ìƒíƒœì˜ ê°€ì¹˜ ì¶”ì •ì¹˜ë¥¼ í™œìš©í•´ í˜„ì¬ì˜ ê°€ì¹˜ë¥¼ ê°±ì‹ í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ëŠ” í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ê³  ë°ì´í„° ìš´ìš© íš¨ìœ¨ì„±ì´ ë†’ì§€ë§Œ, ì¶”ì •ì¹˜ì— ì˜ì¡´í•´ í¸ì°¨ê°€ ì‹¬í•˜ë‹¤. ë°˜ë©´ ëª¬í…Œì¹´ë¥¼ë¡œ ë°©ì‹ì€ ëª¨ë“  ì—í”¼ì†Œë“œê°€ ì¢…ë£Œëœ ì´í›„ ì „ì²´ ë°˜í™˜ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œë‹¤. ì´ ë°©ì‹ì€ í¸ì°¨ëŠ” ì ì§€ë§Œ, ë¶„ì‚°ì´ í¬ë‹¤ëŠ” ë‹¨ì ê³¼ ì—í”¼ì†Œë“œê°€ ì¢…ë£Œëœ ì´í›„ì—ë§Œ ì—…ë°ì´íŠ¸ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.  


---


## Actor Critic
Actor-Critic ì•Œê³ ë¦¬ì¦˜ì€ ì •ì±… ì¤‘ì‹¬ ë°©ë²•ê³¼ ê°€ì¹˜ ì¤‘ì‹¬ ë°©ë²•ì˜ ì¥ì ì„ ê²°í•©í•œ ì ‘ê·¼ìœ¼ë¡œ, ì§ì ‘ì ì¸ ì •ì±… ìµœì í™”ì˜ íš¨ìœ¨ì„±ê³¼ Temporal Difference ê¸°ë°˜ í•™ìŠµì˜ ì•ˆì •ì„±ì„ ë™ì‹œì— ì¶”êµ¬í•œë‹¤. ì´ êµ¬ì¡°ëŠ” ì •ì±…ì„ ê·¼ì‚¬í•˜ëŠ” Actorì™€, í•´ë‹¹ ì •ì±…ì˜ ì„±ëŠ¥ì„ TD ì˜¤ì°¨ ê¸°ë°˜ìœ¼ë¡œ í‰ê°€í•˜ì—¬ í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” Criticìœ¼ë¡œ êµ¬ì„±ëœë‹¤. ActorëŠ” ì •ì±… íŒŒë¼ë¯¸í„°ë¥¼ ê·¸ë˜ë””ì–¸íŠ¸ ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë©°, Criticì€ TD ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ í•™ìŠµí•œë‹¤. ë”°ë¼ì„œ Actor-Criticì€ ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ TD ê¸°ë°˜ìœ¼ë¡œ ê·¼ì‚¬í•œ êµ¬ì¡°ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.

![](../assets/images/RL/ActorCritic/image.png)  

ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ì˜ ì•¡í„° í¬ë¦¬í‹± ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤.  
$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot Q_{w}(s, a) \right]$$

- $\theta$ : ì •ì±… ì‹ ê²½ë§ì˜ íŒŒë¼ë¯¸í„°  
- $w$ : ê°€ì¹˜ ê·¼ì‚¬ ì‹ ê²½ë§ì˜ íŒŒë¼ë¯¸í„°  

ê·¸ëŸ¬ë‚˜ ìœ„ì™€ ê°™ì€ ë°©ì‹ì€ ìƒ˜í”Œë§ ê¸°ë°˜ ì¶”ì •ì—ì„œ ë¶„ì‚°ì´ ì»¤ì ¸ í•™ìŠµì˜ ë¶ˆì•ˆì •ì„±ì„ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ Actor-Criticì— ì–´ë“œë²¤í‹°ì§€ í•¨ìˆ˜ë¥¼ ë„ì…í•œ ë°©ë²•ë¡ ì´ **A2C : Advantage Actor-Critic**ì´ë‹¤.  


### A2C : Advantage Actor-Critic

> Advantage í•¨ìˆ˜ 

$$A(s, a) = Q_w(s, a) - V_v(s)$$

ì–´ë“œë²¤í‹°ì§€ í•¨ìˆ˜ëŠ” ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜  $V_v(s)$ ë¥¼ baselineìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ Q-í•¨ìˆ˜ì˜ ë¶„ì‚°ì„ ì¤„ì´ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤. ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ëŠ” ê° í–‰ë™ì´ ì•„ë‹Œ ìƒíƒœ ê³ ìœ ì˜ ê¸°ëŒ€ ê°€ì¹˜ë¥¼ ê·¼ì‚¬í•˜ê¸° ë•Œë¬¸ì— ë¶„ì‚°ì´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ë‹¤. 
ë”°ë¼ì„œ  A(s, a) ëŠ” ìƒíƒœ-í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜ì—ì„œ ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì œí•¨ìœ¼ë¡œì¨, ê° í–‰ë™ì´ ìƒíƒœì—ì„œ ê°–ëŠ” **ìƒëŒ€ì  ìš°ìˆ˜ì„±(advantage)**ë§Œì„ ê°•ì¡°í•˜ë©° ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ì˜ ë¶„ì‚°ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°ì†Œì‹œí‚¨ë‹¤.  

ì‹¤ì œë¡œ Q-í•¨ìˆ˜ì™€ V-í•¨ìˆ˜ë¥¼ ë³„ë„ë¡œ í•™ìŠµí•˜ì§€ ì•Šê³ , TD ì˜¤ì°¨ë¥¼ ì´ìš©í•˜ì—¬ ì–´ë“œë²¤í‹°ì§€ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤. TD ì˜¤ì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤:


$$\delta_v = R_{t+1} + \gamma V_v(S_{t+1}) - V_v(S_t)$$


ì´ ì˜¤ì°¨ëŠ” ì‹¤ì œ ë³´ìƒê³¼ ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ì˜ ì˜ˆì¸¡ ê°„ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ë©°,  $A(s_t, a_t) \approx \delta_v$ ë¡œ ê°„ì£¼ë  ìˆ˜ ìˆë‹¤.
ì´ë¥¼ í™œìš©í•œ ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:


$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot \delta_v \right]$$

ê²°ê³¼ì ìœ¼ë¡œ, ì–´ë“œë²¤í‹°ì§€ í•¨ìˆ˜ëŠ” ìƒíƒœì— ëŒ€í•œ baselineì¸  $V(s)$ ë¥¼ í™œìš©í•˜ì—¬ gradientì˜ ê¸°ëŒ€ê°’ì€ ìœ ì§€í•˜ë©´ì„œ ë¶„ì‚°ì„ ì¤„ì´ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.  

#### ì¥ë‹¨ì 
A2CëŠ” ì˜¨í´ë¦¬ì‰¬ ë°©ë²•ë¡ ìœ¼ë¡œ ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµê°€ëŠ¥í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ, í•™ìŠµì— ì´ìš©ë˜ëŠ” ìƒ˜í”Œì˜ ìƒê´€ë„ê°€ ë†’ë‹¤ëŠ” í•œê³„ê°€ ì¡´ì¬í•œë‹¤.  


### A3C :  Asynchronous Advantage Actor-Critic 
> ë…¼ë¬¸ ë°”ë¡œê°€ê¸° : [Asynchronous Methods for Deep Reinforcement Learning (2016)](https://arxiv.org/pdf/1602.01783)  

A3CëŠ” ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ **ë¹„ë™ê¸°**ì ìœ¼ë¡œ ì‘ë™ì‹œì¼œ A2Cì˜ ìƒ˜í”Œ ê°„ì˜ ìƒê´€ë„ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.  

A3CëŠ” ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë“¤ì´ ë¶„ë¦¬ëœ í™˜ê²½ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµí•˜ë©°, ì—¬ê¸°ì„œ ë°œìƒí•œ ìƒ˜í”Œì„ ëª¨ì•„ í•™ìŠµí•˜ëŠ” ê¸€ë¡œë²Œ ì‹ ê²½ë§ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.  

![](../assets/images/RL/ActorCritic/image2.png)   

ê¸€ë¡œë²Œ ì‹ ê²½ë§ì€ ì—ì´ì „íŠ¸ë“¤ì´ ë§Œë“¤ì–´ë‚¸ ì—í”¼ì†Œë“œë¥¼ ì¼ì • íƒ€ì„ ìŠ¤í… ë™ì•ˆ ì €ì¥í•˜ê³ , ì €ì¥í•œ ìƒ˜í”Œì„ ì´ìš©í•´ ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•œë‹¤. ê° ì—ì´ì „íŠ¸ì˜ ì‹ ê²½ë§ì€ ì—…ë°ì´íŠ¸ëœ ê¸€ë¡œë²Œ ì‹ ê²½ë§ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœë‹¤. A3CëŠ” ì´ëŸ¬í•œ ì‚¬ì´í´ì„ ë°˜ë³µí•˜ë©° ì˜¨í´ë¦¬ì‰¬ì˜ ì¥ì ì„ ì‚´ë ¤ë©´ì„œ ìƒ˜í”Œ ê°„ì˜ ìƒê´€ë„ë¥¼ ë‚®ì¶˜ë‹¤. 


### A2C êµ¬í˜„ 
![](../assets/images/RL/ActorCritic/image3.png)  

## ì¶œì²˜ 
1. [ê°•í™”í•™ìŠµì—ì„œì˜ bootstrappingì€ ë¬´ì—‡ì¸ê°€? (What is bootstrapping in RL?)](https://cumulu-s.tistory.com/7)  

2. [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)  

3. [ê°•í™” í•™ìŠµ - A3C(Asynchronous Advantage Actor-Critic)](https://wnthqmffhrm.tistory.com/20)  

4. [The Actor-Critic Reinforcement Learning algorithm](https://medium.com/intro-to-artificial-intelligence/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14)  

5. [The idea behind Actor-Critics and how A2C and A3C improve them](https://theaisummer.com/Actor_critics/)  


{% endcapture %}

<!-- ì˜ì–´ ì½˜í…ì¸  -->
{% capture en_content %}

To be continue...

{% endcapture %}

<div id="content-ko" class="lang-content" data-lang="ko">
  {{ ko_content | markdownify }}
</div>

<div id="content-en" class="lang-content" data-lang="en" style="display: none;">
  {{ en_content | markdownify }}
</div>